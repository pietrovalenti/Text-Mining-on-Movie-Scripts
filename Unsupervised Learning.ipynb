{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import rand_score\n",
    "%matplotlib qt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Words Download, run just once\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def preprocText(text):\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "    tokenized_text = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    ##REMOVING STOPWORDS\n",
    "    tokenized_text_without_stopwords = []\n",
    "    for token in tokenized_text:\n",
    "        if token not in stop_words:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            if(len(token)) > 2:\n",
    "                tokenized_text_without_stopwords.append(token)\n",
    "        \n",
    "    return tokenized_text_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading dataset and filtering genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final.csv')\n",
    "genresToKeep = ['Drama','Comedy','Action','Horror','Documentary']\n",
    "df = df[df['Genres'].isin(genresToKeep)]\n",
    "df.Genres.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "rus = RandomUnderSampler(replacement=False, random_state=1234)\n",
    "X, y= rus.fit_sample(X, y)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying preprocessing to extract final corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "corpus = [preprocText(item) for item in X.filmScript]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "model=\"doc2vec\\doc2vec.bin\"\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=5\n",
    "#load model\n",
    "m = Doc2Vec.load(model)\n",
    "\n",
    "#INFER DOCUMENTS\n",
    "Doc2Vect = []\n",
    "for i in range(0,n):\n",
    "    Doc2Vect.append(m.infer_vector(corpus[i], alpha=start_alpha, steps=infer_epoch))\n",
    "Doc2Vect = np.array(Doc2Vect)\n",
    "print(\"Word2Vec Matrix shape: \", Doc2Vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF e TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#50 secs\n",
    "#Frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(analyzer=lambda x:x)\n",
    "dtm_tf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "#tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(dtm_tf.shape)\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=300)\n",
    "TF_SVD = pca.fit_transform(dtm_tf)\n",
    "TFIDF_SVD = pca.fit_transform(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTRAGGO VETTORE GLOVE DEL DOCUMENTO A PARTIRE DAI VETTORI DELLE SIGNOLE PAROLE\n",
    "#Se la parola non Ã¨ presente del vocabolario passo array di 0\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'glove.6B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = []\n",
    "for document in corpus:\n",
    "    documentRepr = []\n",
    "    for word in document:\n",
    "        try:\n",
    "            documentRepr.append(model.get_vector(word))\n",
    "        except Exception:\n",
    "            documentRepr.append( np.zeros(300))\n",
    "    GloVe.append( np.mean(np.array(documentRepr),axis=0) )\n",
    "GloVe = np.array(GloVe)\n",
    "GloVe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passare come origin 'svdtf','svdtfidf', 'doc2vec' o 'glove' in base a quale clustering e pca si vuole fare\n",
    "origin = 'doc2vec'\n",
    "clust_n = 5\n",
    "\n",
    "#SVD TF\n",
    "if origin=='svdtf':\n",
    "    xx = MinMaxScaler().fit_transform(TF_SVD)\n",
    "#SVD TF\n",
    "elif origin=='svdtfidf':\n",
    "    xx = MinMaxScaler().fit_transform(TFIDF_SVD)\n",
    "#Doc2Vec\n",
    "elif origin=='doc2vec':\n",
    "    xx = MinMaxScaler().fit_transform(Doc2Vect)\n",
    "#GloVe\n",
    "elif origin=='glove':\n",
    "    xx = MinMaxScaler().fit_transform(GloVe)\n",
    "\n",
    "\n",
    "\n",
    "#KMEANS\n",
    "clustering = KMeans(n_clusters=clust_n).fit(xx)\n",
    "#clustering = AgglomerativeClustering(n_clusters=clust_n).fit(xx)\n",
    "#clustering = SpectralClustering(n_clusters=clust_n, affinity='nearest_neighbors',assign_labels='discretize',n_neighbors = 100).fit(xx)\n",
    "\n",
    "print(\"Rand Index: %0.3f\"\n",
    "      % metrics.rand_score(y, clustering.labels_))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(y, clustering.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "  % metrics.silhouette_score(xx, clustering.labels_))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPCA(x,y,label,clust_n,title):\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    sns.scatterplot(\n",
    "        x=x, y=y,\n",
    "        hue=label,\n",
    "        palette=sns.color_palette(\"hls\", clust_n),\n",
    "        data=tsneDF,\n",
    "        legend=\"full\",\n",
    "        alpha=0.75,\n",
    "        s=40\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "if origin=='svdtf':\n",
    "    principalComponents = TF_SVD\n",
    "\n",
    "elif origin=='svdtfidf':\n",
    "    principalComponents = TFIDF_SVD\n",
    "\n",
    "elif origin=='doc2vec':\n",
    "    principalComponents = Doc2Vect\n",
    "    \n",
    "elif origin=='glove':\n",
    "    principalComponents = GloVe\n",
    "    \n",
    "##PCA\n",
    "tsne = TSNE(n_components=2, verbose=2, perplexity=100, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(principalComponents)\n",
    "tsneDF = pd.DataFrame(data = tsne_results, columns = ['PC1', 'PC2'])\n",
    "tsneDF['Label'] = clustering.labels_\n",
    "tsneDF['TrueLabel'] = y\n",
    "\n",
    "#Plotting PCA visualization with cluster label and true labels\n",
    "plotPCA(tsneDF['PC1'],tsneDF['PC2'],tsneDF['Label'],clust_n,str(origin + ': Clustering Labels: '))\n",
    "plotPCA(tsneDF['PC1'],tsneDF['PC2'],tsneDF['TrueLabel'],clust_n,str(origin + ': True Labels: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Purity values for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterGroups = tsneDF.groupby(['Label','TrueLabel']).PC1.count().reset_index()\n",
    "sums = ClusterGroups.groupby('Label').sum().PC1.tolist()\n",
    "\n",
    "PercInClust = []\n",
    "for index, row in ClusterGroups.iterrows():\n",
    "    PercInClust.append(row['PC1'] / sums[row['Label']])\n",
    "ClusterGroups['Purity'] = [round(num, 2) for num in PercInClust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainGenre = ClusterGroups.loc[ClusterGroups.groupby(['Label'])['Purity'].idxmax()].rename(columns={'Label':'Cluster','TrueLabel':'MainGenre','PC1':'Count'})\n",
    "mainGenre['total'] = sums\n",
    "mainGenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting full confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.pivot_table(ClusterGroups, index = 'Label', columns = 'TrueLabel', values = 'PC1')\n",
    "Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
